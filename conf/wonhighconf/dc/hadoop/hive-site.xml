<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://172.17.208.15:3306/dc_hive?createDatabaseIfNotExist=true&amp;amp;characterEncoding=UTF-8</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>dc_hive</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>dc_hive</value>
    </property>
    <!-- local or remote metastore (removed as of Hive 0.10: If hive.metastore.uris is empty local mode is assumed, remote otherwise)-->
   <!--
    <property>
        <name>hive.metastore.local</name>
        <value>true</value>
    </property>
    -->
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>hdfs://nn209003:9000/hive/warehouse</value>
    </property>
    <!-- Try to generate a map-only job for merging files if CombineHiveInputFormat is supported. (This configuration property was removed in release 0.11.0.) -->
    <!--
    <property>
        <name>hive.mergejob.maponly</name>
        <value>true</value>
    </property>
    -->
    <property>
        <name>hive.merge.mapfiles</name>
        <value>true</value>
    </property>
    <property>
        <name>hive.merge.rcfile.block.level</name>
        <value>true</value>
    </property>
    <property>
        <name>hive.merge.size.per.task</name>
        <value>90000000</value>
    </property>
    <property>
        <name>hive.input.format</name>
        <value>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</value>
    </property>
    <property>
        <name>hive.merge.smallfiles.avgsize</name>
        <value>16000000</value>
    </property>
    <property>
        <name>hive.merge.mapredfiles</name>
        <value>true</value>
    </property>
    <property>
        <name>mapred.output.compress</name>
        <value>true</value>
    </property>
    <property>
        <name>mapred.output.compression.type</name>
        <value>BLOCK</value>
    </property>
    <property>
        <name>mapred.output.compression.codec</name>
        <value>org.apache.hadoop.io.compress.Lz4Codec</value>
    </property>
    <property>
        <name>mapred.map.output.compression.codec</name>
        <value>org.apache.hadoop.io.compress.Lz4Codec</value>
    </property>
    <property>
        <name>hive.security.authorization.enabled</name>
        <value>false</value>
        <description>enable or disable the hive client authorization</description>
    </property>
    <property>
        <name>hive.security.authorization.createtable.owner.grants</name>
        <value>ALL</value>
        <description>the privileges automatically granted to owner whereever a table gets created.</description>
    </property>
    <property>
        <name>hive.server2.authentication</name>
        <value>KERBEROS</value>
        <description>
            Client authentication types.
            NONE: no authentication check
            LDAP: LDAP/AD based authentication
            KERBEROS: Kerberos/GSSAPI authentication
            CUSTOM: Custom authentication provider
            (Use with property hive.server2.custom.authentication.class)
        </description>
    </property>
    <property>
        <name>hive.server2.authentication.kerberos.principal</name>
        <value>hive/_HOST@WONHIGH.CN</value>
    </property>
    <property>
        <name>hive.server2.authentication.kerberos.keytab</name>
        <value>/data/conf/hadoop/kerberos/keys/hive.keytab</value>
    </property>
    <property>
        <name>hadoop.proxyuser.hive.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.hive.groups</name>
        <value>*</value>
    </property>
    <property>
        <name>hive.server2.enable.doAs</name>
        <value>false</value>
    </property>
<!--    <property>
        <name>hive.enforce.bucketing</name>
        <value>true</value>
    </property>-->
    <property>
        <name>hive.metastore.client.socket.timeout</name>
        <value>600</value>
    </property>
    <!-- added by wang.w
    <property>
        <name>hive.server2.thrift.min.worker.threads</name>
        <value>20</value>
        <description>Minimum number of worker threads, default 5</description>
    </property>
    <property>
        <name>hive.server2.thrift.max.worker.threads</name>
        <value>2000</value>
        <description>Maximum number of worker threads, default 500</description>
    </property>
    <property>
        <name>hive.server2.thrift.port</name>
        <value>10000</value>
        <description>TCP port number to listen on, default 10000</description>
    </property>
    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>dn012016</value>
        <description>TCP interface to bind to</description>
    </property>
    <property>
        <name>hive.server2.thrift.worker.keepalive.time</name>
        <value>600</value>
        <description>Keepalive time (in seconds) for an idle worker thread. When number of workers > min workers, excess threads are killed after this time interval</description>
    </property>
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>hdfs://nn012018:9000/hive/warehouse</value>
        <description>location of default database for the warehouse</description>
    </property>
    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://nn012018:9088</value>
        <description>Hive connects to one of these URIs to make metadata requests to a remote Metastore (comma separated list of URIs)</description>
    </property>
    -->
    <!-- Added by Lifeng,0.6.2_SP1 -->
   
    <property>
        <name>hive.exec.dynamic.partition</name>
        <value>true</value>
    </property>
    <property>
         <name>hive.exec.dynamic.partition.mode</name>
        <value>nonstrict</value>
    </property>
    <!-- Added by Qiuzhuang to enlarge reduces to try to avoid GC -->
    <property>
        <name>hive.exec.reducers.bytes.per.reducer</name>
        <!--   1000000000 -->
        <value>500000000</value>
        <description>size per reducer.The default is 1G, i.e if the input size is 10G, it will use 10 reducers.</description>
    </property>
    <property>
      <name>mapred.cluster.map.memory.mb</name>
      <value>1324</value>
    </property>
    <property>
      <name>mapred.cluster.reduce.memory.mb</name>
      <value>1324</value>
    </property>
    <property>
      <name>mapred.child.java.opts</name>
      <value>-Xmx1000m</value>
    </property>
    <property>
       <name>fs.hdfs.impl.disable.cache</name>
       <value>true</value>
    </property>
    <property>
       <name>fs.file.impl.disable.cache</name>
       <value>true</value>
    </property>
    <!-- add by Qiuzhuang for partitions-->
    <property>
       <name>hive.exec.max.dynamic.partitions.pernode</name>
       <value>2000</value>
    </property>
    <property>
      <name>hive.exec.max.dynamic.partitions</name>
      <value>25000</value>
    </property>

    <property>
     <name>hive.txn.manager</name>
     <value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>
    </property>

    <property>
     <name>hive.compactor.initiator.on</name>
     <value>true</value>
    </property>

    <property>
     <name>hive.compactor.worker.threads</name>
     <value>1</value>
    </property>

    <property>
     <name>hive.support.concurrency</name>
     <value>true</value>
    </property>

    <property>
     <name>hive.enforce.bucketing</name>
     <value>true</value>
    </property>

    <property>
     <name>hive.optimize.index.filter</name>
     <value>true</value>
    </property>

    <property>
     <name>hive.optimize.ppd</name>
     <value>true</value>
    </property>

    <property>
     <name>hive.merge.orcfile.stripe.level</name>
     <value>true</value>
    </property>

    <property>
     <name>hive.enforce.sorting</name>
     <value>false</value>
    </property>

    <!-- hive optimizations -->
    <property>
     <name>hive.vectorized.execution.enabled</name>
     <value>true</value>
    </property>

    <property>
     <name>hive.vectorized.execution.reduce.enabled</name>
     <value>true</value>
    </property>

    <property>
      <name>hive.optimize.bucketmapjoin</name>
      <value>true</value>
    </property>
</configuration>
