log.home=/data/logs/wonhigh/dc/client/
#jmx.host=172.17.234.3
jmx.host=127.0.0.1
jmx.port=6088
dc.sqoop.input.isOpenMapNum=0
dc.task.freq.seconds=86400
dc.sqoop.bindir=/home/hive/bindir
dc.sqoop.outdir=/home/hive/outdir
dc.sqoop.fields.terminated=\\001
dc.sqoop.lines.terminated=\\n
dc.sqoop.import.map.num=1
dc.sqoop.export.map.num=5
dc.sqoop.input.null.string=\\\\N
dc.sqoop.input.null.not.string=\\\\N
dc.sqoop.null.string=\\\\N
dc.sqoop.null.not.string=\\\\N
dc.sqoop.split.by=id
dc.sqoop.export.dir.prefix=/dev_hive/warehouse/
dc.date.format.default=yyyy-MM-dd HH:mm:ss
dc.import.syncEndTime.advance=200
dc.import.start.syncEndTime=2015-08-07 00:00:00
rinse.repeat.data.open=0
select.status.days=33
dc.export.pg.span=30
dc.pg.minDataPreMonth=6
create.time.list=create_time,createtm,create_date
update.time.list=update_time,edittm,update_date
is.balance.split=0
delDataRemove.handler.thread.num=5
is.open.export.batch=1
delDataRemove.handler.delData.tbNames=retail_gms_sync_deldata
is.open.export.direct=1
#bdp-dht  debug\u6A21\u5F0F\u662F\u5426\u5F00\u542F \u9ED8\u8BA4\u5F00\u542F
bdp-dht.debug=false
dc.delctl.isopen=false
config.scantime=10000

# the config of jdbc connection for hive 
jdbc.hive.user=hive
jdbc.hive.password=wonhigh@015
jdbc.hive.timeout=600
select.history.month.num=12
clnd.pre.month.num=12
#kerberos right for hiveserver2
#dc.hive.principal=hive/dev-app-003@WONHIGH.CN
dc.hive.principal=hive/dev-dc-mq-01@WONHIGH.CN
dc.spark.hive.principal=hive/dc-spark-001@WONHIGH.CN
##\u76EE\u6807\u5BFC\u51FA\u5E93id \u914D\u7F6E
target.db.id=4
#\u591A\u4E2A\u7528\u6237\u7528\u9017\u53F7\u9694\u5F00
mail.reciver.addr=jiang.pl@wonhigh.cn,wang.lei@wonhigh.cn,zhang.sl@wonhigh.cn,wang.fl@wonhigh.cn,wang.zq@wonhigh.cn,chen.yz@wonhigh.cn,wang.w@wonhigh.cn
mail.ccreciver.addr=
mail.reciver.title=this is information for bdp-dht  dev

##activemq
#activemq.host=failover:(tcp://172.17.210.123:3046)?initialReconnectDelay=30000&maxReconnectAttempts=-1&timeout=30000
client.server.mq.url=failover:(tcp://10.234.6.192:3046,tcp://10.234.6.193:3046,tcp://10.234.6.194:3046)?initialReconnectDelay=10000&maxReconnectAttempts=-1
activemq.username=admin
activemq.password=admin

#send email parameters
mail.host=smtp.wonhigh.cn
mail.username=SchedulerException@wonhigh.cn
mail.password=bl150604
mail.port=25
mail.encoding=UTF-8
mail.smtp.auth=true
mail.smtp.timeout=25000

#copy sqoop
#is.open.export.direct=1
is.open.slave.export.map=1

#dc.import.params=mapred.job.name=sqoop,mapreduce.job.queuename=b
dc.import.params=mapred.job.name=sqoop
dc.export.params=\-\-escaped-by=\\006,\-\-enclosed-by=\\006

#Password encryption key value
pwd.encryption.key=dcclient

pwd.encryption.on.off=true


clnd.cast.thl.open=true
dc.data.synVerification.trace=1
dc.data.synVerification.date=60
dc.data.synVerification.standard=5
tab.dup.primary.config.key=365

#bdp-hadoop  params
bdp.fs.default=hdfs://nn209003:9000


cdh.hive.path=webhdfs://172.17.194.20:50070/user/hive/warehouse/gtp_dc.db/
bdp.hive.path=hdfs://nn209003:9000/dev_hive/warehouse/gtp.db/
special.table=gyl_wms_city_bill_im_import_src,gyl_wms_city_bill_im_import_dtl_src,blf1_bl_wms_check_dtl_src,blf1_bl_wms_check_src,blf1_bl_wms_deliver_dtl_src
#set hadoop job name


#set Global hadoop.thl.params
dc.hadoop.thl.params=mapreduce.map.memory.mb=1024,mapreduce.map.java.opts=-Xmx768m,mapreduce.reduce.memory.mb=2048,mapreduce.reduce.java.opts=-Xmx1536m,io.sort.mb=500,hive.exec.compress.intermediate=true
#set Global hadoop.clnd.params
dc.hadoop.clnd.params=mapreduce.map.memory.mb=4096,mapreduce.map.java.opts=-Xmx3048m,mapreduce.reduce.memory.mb=4096,mapreduce.reduce.java.opts=-Xmx3073m,io.sort.mb=500,hive.exec.compress.intermediate=true


#--------重复了，取哪个为准----
#bdp-hadoop  params
#bdp.fs.default=hdfs://nn209003:9000


#cdh.hive.path=webhdfs://172.17.194.20:50070/user/hive/warehouse/gtp_dc.db/
#cdh.hive.path=/user/hive/warehouse/gtp_dc.db/
#bdp.hive.path=hdfs://nn209003:9000/dev_hive/warehouse/gtp.db/
#--------重复了，取哪个为准----

#kerberos  params
bdp.hdfs.key=user/172.17.222.139@WONHIGH.CN
#bdp.hdfs.key=deng_yb@WONHIGH.COM
bdp.hdfs.keytab.dir=C:/temp/krb5cache/user.keytab
#bdp.hdfs.keytab.dir=C:/temp/krb5cache/deng_yb.keytab

#set sentry 
clnd.has.sentry=1

cdc.table.list=bi_mdm_transaction_history_log_src,blf1_transaction_history_log_src,ck_test_richy_transaction_history_log_src,dc_rest_client_transaction_history_log_src,gyl_tms_transaction_history_log_src,gyl_wms_city_global_transaction_history_log_src,gyl_wms_city_transaction_history_log_src,gyl_wms_transaction_history_log_src,gyl_wms_z_transaction_history_log_src,hrms_global_transaction_history_log_src,hrms_transaction_history_log_src,miu_transaction_history_log_src,rest_client_transaction_history_log_src,retail_fas_transaction_history_log_src,retail_gms_transaction_history_log_src,retail_mdm_transaction_history_log_src,retail_mps_transaction_history_log_src,retail_pms_transaction_history_log_src,retail_pos_transaction_history_log_src

#redis.host=10.234.6.153
redis.host=10.9.251.34
#redis.host=10.240.12.250
redis.port=6379

#--------获取yarn资源管理配置信息--------#
yarn.host=10.9.251.31
yarn.host.port=8088

# --------子定义线程池配置-----
define.thread.coreThread=20
define.thread.maxThread=40
define.thread.keepAlive=120
define.thread.queueCapacity=1000

#-----druid线程池参数配置
#连接url
hive.jdbc.src=jdbc:hive2://10.9.251.32:10000/dc_src
hive.jdbc.ods=jdbc:hive2://10.9.251.32:10000/dc_ods
#连接池类型
hive.type=com.alibaba.druid.pool.DruidDataSource
#驱动类型，可不配置，由url推出驱动类型
hive.driver.class.name=org.apache.hive.jdbc.HiveDriver
hive.user=hive
hive.password=123456
#最大连接数
hive.max.active=100
#初始化连接数
hive.initialSize=4
#连接时最大等待时间
hive.maxWait=60000
#	最小连接池数量
hive.minIdle=1
hive.timeBetweenEvictionRunsMillis=60000
hive.minEvictableIdleTimeMillis=300000
hive.testWhileIdle=true
hive.validationQuery=SELECT 1
hive.testOnBorrow=false
hive.testOnReturn=false
hive.poolPreparedStatements=true
hive.maxOpenPreparedStatements=50
hive.removeAbandoned=true
hive.removeAbandonedTimeout=180

